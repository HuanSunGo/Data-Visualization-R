---
title: "GR5063 Data Visualization"
subtitle: "h03 Text Mining Kickstarter Projects Huan Sun"
output:
  html_document:
    df_print: paged
---

## Importing the Dataset \& Packages
```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(DT)
library(viridis)
library(hrbrthemes)
library(tm)
library(stringr)
library(SnowballC)
library(tidytext)
library(wordcloud)
library(quanteda)
KickStarter = read.csv('kickstarter_projects_2021-05.csv')
```

## Overview

Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowd funding platform focused on creativity.  The company's stated mission is to "help bring creative projects to life". 

Kickstarter has reportedly received almost $6 billion in pledges from 20 million backers to fund more than 200,000 creative projects, such as films, music, stage shows, comics, journalism, video games, technology and food-related projects.

For this assignment, I am asking you to analyze the descriptions of kickstarter projects to identify commonalities of successful (and unsuccessful projects) using the text mining techniques we covered in the past two lectures. 

## Data

The dataset for this assignment is taken from [webroboto.io â€˜s repository](https://webrobots.io/kickstarter-datasets/). They developed a scrapper robot that crawls all Kickstarter projects monthly since 2009. I noticed that the most recent crawls appear to be incomplete, so we will take data from the last complete crawl on 2021-05-17.

To simplify your task, I have downloaded the files and partially cleaned the scraped data. In particular, I converted several JSON columns, corrected some obvious data issues, and removed some variables that are not of interest (or missing frequently), and removed some duplicated project entries. I have also  subsetted the data to only contain projects with locations set to the United States (to have only English language and USD denominated projects). Some data issues surely remain, so please adjust as you find it necessary to complete the analysis. 

The data is contained in the file `kickstarter_projects_2021_05.csv` and contains about 131k projects and about 20 variables.

## Tasks for the Assignment

### 1. Identifying Successful Projects

#### a) Success by Category

There are several ways to identify success of a project:  
  - State (`state`): Whether a campaign was successful or not.   
  - Pledged Amount (`pledged`)   
  - Achievement Ratio: The variable `achievement_ratio` is calculating the percentage of the original monetary `goal` reached by the actual amount `pledged` (that is `pledged`\\`goal` *100).    
  - Number of backers (`backers_count`)  
  - How quickly the goal was reached (difference between `launched_at` and `state_changed_at`) for those campaigns that were successful.  

Use **two** of these measures to visually summarize which categories were most successful in attracting funding on kickStarter. Briefly summarize your findings.


##### Answer
  - Metrics 1: 
I'll use `state` to summarize which `top_category` have the highest success rates. 
```{r}
# check for the status value counts
dplyr::count(KickStarter,state,sort=TRUE)
```

As for clearer definition and operations later, here I only count the `success` and `failed` into consideration.
```{r message=FALSE, warning=FALSE}
# group data by `top_category` and calculate percentage of success
state <- KickStarter %>%
  filter(state == 'failed' | state == 'successful') %>%
  group_by(top_category,state)%>%
  summarise(count = n()) %>%
  mutate(ratio = round(count / sum(count),2))%>%
  filter(state=="successful")%>%
  select(top_category,ratio)%>%
  arrange(desc(ratio))

state
```

```{r}
# create bar plot
ggplot(data = state ,aes(x=reorder(top_category,ratio),y=ratio))+
  geom_bar(stat="identity",alpha=0.4)+
  geom_text(aes(label = ratio),colors='b')+
  theme(legend.position = "none")+
  labs(y="Success Ratio",x= 'Top Category',title="Top Categories Succeeding in KickStarter")+
  coord_flip()+
  theme_ipsum()
```
  - Metrics 2: 
I'll check on the pledged ratio, to measure the level of success. 
```{r}
# calculate the achievement ratio by dividing the amount being pledged to the amount of goal
time <- KickStarter%>%
  select(top_category,pledged,goal)%>%
  mutate(achievement_ratio=round(pledged/goal *100,2))%>%
  group_by(top_category) %>% 
  summarise(mean(achievement_ratio))%>%
  rename(average_achievement_ratio ='mean(achievement_ratio)')%>%
  arrange(desc(average_achievement_ratio))
  
time
```
```{r}
# create a datatable for viz
datatable(time, rownames=FALSE, colnames=c("Top Category","Average Achievement Ratio"), caption=htmltools::tags$caption("Top Categories Succeeding in KickStarter"), options=list(autoWidth = TRUE, dom = "ft", pageLength=10), filter = list(position="top"))
```

We observed that while measuring by `success ratio`, the category of *comic* stands the highest chance, while if change the metric to `pledge ratio`, now *music* is the category that being most successful. What's more, both metrics suggest that there's huge discrepancy in terms of the success of each category. 

#### **BONUS ONLY:** b) Success by Location

Now, use the location information to calculate the total number of successful projects by state (if you are ambitious, normalize by population). Also, identify the Top 50 "innovative" cities in the U.S. (by whatever measure you find plausible). Provide a leaflet map showing the most innovative states and cities in the U.S. on a single map based on these information.

```{r}
# calculate the total number of successful projects by state 
successful_case_by_state <- KickStarter %>% 
  filter(state=="successful") %>% 
  group_by(location_state) %>% 
  count(location_state) %>% 
  arrange(desc(n)) %>% 
  rename(`Total Successful Cases`= n)

successful_case_by_city <- KickStarter %>% 
  filter(state=="successful") %>%
  select(location_state,location_town) %>%
  group_by(location_town) %>% 
  mutate(n=n())%>% 
  arrange(desc(n)) %>% 
  rename(`Total Successful Cases` = n) %>% 
  distinct(location_town,.keep_all = TRUE)%>%
  head(50)

datatable(successful_case_by_city)
```




### 2. Writing your success story

Each project contains a `blurb` -- a short description of the project. While not the full description of the project, the short headline is arguably important for inducing interest in the project (and ultimately popularity and success). Let's analyze the text.

#### a) Cleaning the Text and Word Cloud

To reduce the time for analysis, select the 1000 most successful projects and a sample of 1000 unsuccessful projects (by a metric of your choice). Use the cleaning functions introduced in lecture (or write your own in addition) to remove unnecessary words (stop words), syntax, punctuation, numbers, white space etc. Note, that many projects use their own unique brand names in upper cases, so try to remove these fully capitalized words as well (since we are aiming to identify common words across descriptions). Create a document-term-matrix.

Provide a word cloud of the most frequent or important words (your choice which frequency measure you choose) among the most successful projects.


##### Answer
I choose most successful projects by the metric of backers count, and randomly choose 1000 unsuccessful projects. 
```{r}
# filter the failure one first 
failure_sample = KickStarter %>% filter(state=='failed')  

# create the sample 
set.seed(2022)
text_sample = rbind(head(KickStarter %>% 
                      filter(state=='successful') %>%
                      arrange(desc(backers_count)), 1000), 
                 failure_sample[sample(nrow(failure_sample), 1000), ])
```

Clean up the text.
```{r}
# remove punctuation
text_sample$clean_blurb = removePunctuation(text_sample$blurb)

# remove numbers
text_sample$clean_blurb = removeNumbers(text_sample$clean_blurb)

# remove fully-capitalized words 
text_sample$clean_blurb = str_trim(gsub("\\b[A-Z]+\\b","", text_sample$clean_blurb))

# remove white space
text_sample$clean_blurb = stripWhitespace(text_sample$clean_blurb)

# lowercase all
text_sample$clean_blurb = tolower(text_sample$clean_blurb)

# remove stop-words
text_sample$clean_blurb = str_trim(removeWords(text_sample$clean_blurb, stopwords('en')))

# stemming
text_sample$clean_blurb = stemDocument(text_sample$clean_blurb)
```

Check the cleaning process.
```{r}
text_sample$blurb[1]
```
```{r}
text_sample$clean_blurb[1]
```


Create a document term matrix using the `clean_blurb`.
```{r}
# bring the clean_blurb into a dataframe 
df_source = data.frame(doc_id=text_sample$id, text = text_sample$clean_blurb, stringsAsFactors = FALSE)

# create a DataframeSource first, then convert to Corpus object (in the tm() package)
df_corpus = VCorpus(DataframeSource(df_source))

# create DTM
text_dtm = DocumentTermMatrix(df_corpus)

# review a portion of the matrix
as.matrix(text_dtm)[1:5,1:10]
```

```{r}
# convert the matrix back to df
library(tidytext)
text_td = tidy(text_dtm)
text_td$document = as.numeric(text_td$document)

# bind TF, DF, IDF frequencies
text_td <-  text_td %>%
  bind_tf_idf(term, document, count)

# merge the 2 documents
text = left_join(text_td, text_sample %>% select(id, state, clean_blurb), 
                        by=c('document' = 'id'))
```


Word cloud of important words.
```{r echo=TRUE, message=FALSE, warning=FALSE}
# set seed
set.seed(2022)

# create dataframe
text_successful <- text %>% 
  filter(state == 'successful')

# Produce word cloud on success  projects only
wordcloud(text_successful$term, text_successful$tf, 
          max.words = 100, colors = brewer.pal(8, "PuOr"),
          scale = c(3, 0.2))
```

#### b) Success in words

Provide a pyramid plot to show how the words between successful and unsuccessful projects differ in frequency. A selection of 10 - 20 top words is sufficient here.

##### Answer
```{r message=FALSE, warning=FALSE}
# collect top 20 words according to frequency
top_20_words <- text %>% 
  group_by(term) %>%
  summarise(n = sum(count)) %>%
  arrange(desc(n)) %>%
  head(20)

# filter with the collected words 
SampleTop20 <- text %>% 
  filter(term %in% top_20_words$term) %>%
  group_by(term, state) %>%
  summarise(frequency = sum(count))
```


```{r message=FALSE, warning=FALSE}
# pyramid Plot
ggplot(SampleTop20, aes(x = reorder(term, frequency), fill = state))+
  geom_bar(data=SampleTop20 %>% filter(state=='successful'), 
           aes(y=frequency), stat='identity')+
  geom_bar(data=SampleTop20 %>% filter(state=='failed'), 
           aes(y=-frequency), stat='identity')+
  scale_fill_brewer(palette = "Set1")+
  labs(x='Term', y='Frequency',
       title="Top 20 Most Frequent Words in Blurbs")+
  scale_y_continuous(labels=abs)+
  coord_flip()+
  theme_ipsum()
```

The plot shows that words like game, new, are frequent in successful stories, while help, create are most frequent in failed projects.


#### c) Simplicity as a virtue

These blurbs are short in length (max. 150 characters) but let's see whether brevity and simplicity still matters. Calculate a readability measure (Flesh Reading Ease, Flesh Kincaid or any other comparable measure) for the texts. Visualize the relationship between the readability measure and one of the measures of success. Briefly comment on your finding.


```{r}
# use  Flesh-Kincaid statistic
require(quanteda.textstats)
df_source2 = data.frame(doc_id=text_sample$id, text = text_sample$blurb, stringsAsFactors = FALSE)
sample_fre = textstat_readability(corpus(df_source2), measure=c('Flesch.Kincaid'))
sample_fre$document = as.numeric(sample_fre$document)
sample_fre = left_join(text_sample, sample_fre, by=c('id' = 'document'))
```

```{r}
# boxplot
ggplot(sample_fre)+
  geom_boxplot(aes(x=state, y=Flesch.Kincaid, fill=state))+
  scale_fill_brewer(palette = "Set1")+
  labs(x='State', y='Flesch-Kincaid Reading Ease',
       title="Flesch-Kincaid Reading Ease of Blurbs")+
  theme_ipsum()
```
The plot above shows no significant difference in the level of reading difficult for succeed or failed project.

### 3. Sentiment

Now, let's check whether the use of positive / negative words or specific emotions helps a project to be successful. 

#### a) Stay positive

Calculate the tone of each text based on the positive and negative words that are being used. You can rely on the Hu & Liu dictionary provided in lecture or use the Bing dictionary contained in the tidytext package (`tidytext::sentiments`). Visualize the relationship between tone of the document and success. Briefly comment.


```{r}
# getting in Hu-Liu dictionary
pos <- read.table('positive-words.txt', as.is=T)
neg <- read.table('negative-words.txt', as.is=T)

# get the data, use the text_sample that had 2000 lines
text_sentiment <- text_sample %>%
  select(backers_count,state,clean_blurb)

text_sentiment

```

```{r}
# function for sentiment analysis
sentiment <- function(words){
  require(quanteda)
  tok <- quanteda::tokens(words)
  pos.count <- sum(tok[[1]]%in%pos[,1])
  neg.count <- sum(tok[[1]]%in%neg[,1])
  out <- (pos.count - neg.count)/(pos.count+neg.count)
  return(out)
}

# test the function
sentiment(text_sentiment$clean_blurb[10])
```

```{r}
# add the sentiment score back 
mylist <- c()

for(i in text_sentiment$clean_blurb){
  mylist<-c(mylist,sentiment(i))
} 

text_sentiment$tone <- mylist
text_sentiment[is.na(text_sentiment)] <- 0
text_sentiment
```

```{r}
# visualize the sentiment score 
ggplot(text_sentiment, aes(x = tone, y = backers_count, size = backers_count,color = tone)) +
  geom_smooth(alpha = 0.3) +
  geom_point(alpha = 0.3)+
  ggtitle("Backers Count and Tone") +
  xlab("Tone") + 
  ylab("Backers Count") +
  scale_y_continuous(breaks = seq(0, 100000, 20000)) +
  theme(plot.title = element_text(hjust = 0.5)) +
  theme_ipsum()
  
```

The graph showed that negative tone is more likely to cause less backers, yet having positive tones  doesn't guarantee a higher number of the backers.  

#### b) Stay positive
Segregate all 2,000 blurbs into positive and negative texts based on their polarity score calculated in step (a). Now, collapse the positive and negative texts into two larger documents. Create a document-term-matrix based on this collapsed set of two documents. Generate a comparison cloud showing the most-frequent positive and negative words.

```{r}
# filter for positive and negative text document 
text_sentiment$sentiment_cat <- ifelse(text_sentiment$tone > 0, 'positive', 'negative')

# concatenate the text them
text_sentiment_concat <- text_sentiment %>% 
  group_by(sentiment_cat) %>%
  summarise(text=paste(clean_blurb, collapse=''))

```

Create the document-term-matrix. 
```{r}
# create the dtm
colnames(text_sentiment_concat) <- c('doc_id', 'text') # to avoid the error
text_sentiment_concat_corp = VCorpus(DataframeSource(text_sentiment_concat))

# Create DTM
text_sentiment_concat_dtm = as.matrix(TermDocumentMatrix(text_sentiment_concat_corp))
```

```{r}
# create the comparison cloud 
comparison.cloud(text_sentiment_concat_dtm, colors=c("red", "steelblue"), 
                 scale=c(3, 0.2), max.words = 100,
                 title.size= 3)
```
#### c) Get in their mind
Now, use the NRC Word-Emotion Association Lexicon in the tidytext package to identify a larger set of emotions (anger, anticipation, disgust, fear, joy, sadness, surprise, trust). Again, visualize the relationship between the use of words from these categories and success. What is your finding?

```{r}
# nrc lexicon
nrc = get_sentiments('nrc')
```
```{r}
# build dictionary
NRC <-
  nrc%>%
  filter(sentiment != c("negative","positive"))

anger <- NRC%>%
  filter(sentiment == "anger")%>%
  select(word)%>%
  as.data.frame() 

anticipation <- NRC%>%
  filter(sentiment == "anticipation")%>%
  (word)%>%
  as.data.frame() 

disgust <- NRC%>%
  filter(sentiment == "disgust")%>%
  select(word)%>%
  as.data.frame() 

fear <- NRC%>%
  filter(sentiment == "fear")%>%
  select(word) %>%
  as.data.frame() 

joy <- NRC%>%
  filter(sentiment == "joy")%>%
  select(word)%>%
  as.data.frame()

sadness <- NRC%>%
  filter(sentiment == "sadness")%>%
  select(word)%>%
  as.data.frame()   

surprise <- NRC%>%
  filter(sentiment == "surprise")%>%
  select(word)%>%
  as.data.frame()  

trust <- NRC%>%
  filter(sentiment == "trust")%>%
  select(word)%>%
  as.data.frame()  
```

```{r}
 # come with new function for broader sentiment analysis
NRCsentiment <- function(words){
  require(quanteda)
  tok <- quanteda::tokens(words)
  anger.count <- sum(tok[[1]]%in%anger[,1])
  anticipation.count <- sum(tok[[1]]%in%anticipation[,1])
  disgust.count <- sum(tok[[1]]%in%disgust[,1])
  fear.count <- sum(tok[[1]]%in%fear[,1])
  joy.count <- sum(tok[[1]]%in%joy[,1])
  sadness.count <- sum(tok[[1]]%in%sadness[,1])
  surprise.count <- sum(tok[[1]]%in%surprise[,1])
  trust.count <- sum(tok[[1]]%in%trust[,1])
  out <- c(anger.count, anticipation.count, disgust.count, fear.count, joy.count,sadness.count,surprise.count,trust.count)
  return(out)
}
```

```{r}
# chose backers count as the matrix for success 
text_sentiment_success <- text_sentiment%>%
  rename("text"="clean_blurb")%>%
  filter(state=="successful")%>%
  select(text)

text_successful<-paste(text_sentiment_success, collapse=" ")

text_sentiment_failed <- text_sentiment%>%
  rename("text"="clean_blurb")%>%
  filter(state=="failed")%>%
  select(text)

text_failed<-paste(text_sentiment_failed, collapse=" ")

```

```{r}
# get the new sentiment result 
success <- NRCsentiment(text_successful)
fail <- NRCsentiment(text_failed)

# revert the result into dataframe
emotions <- c("anger","anticipation", "disgust", "fear", "joy", "sadness", "surprise", "trust")
emotions_df <- data.frame(emotions, success, fail)%>% 
  as.tibble()
emotions_df <- gather(emotions_df,"state","count",-emotions)
```

```{r}
# plot the difference 
ggplot(emotions_df,aes(x=emotions, y=count,fill=state)) +
  geom_bar(stat="identity",position="dodge", width=0.4, alpha = 0.8) +
  scale_fill_manual(values=c("red", "steelblue"))+
  ggtitle("Relationship: word use and project success") +
  theme(plot.title = element_text(hjust = 0.5)) +
  coord_flip()+
  theme_ipsum()
```

Surprisingly, the positive emotions like trust, joy,and anticipation are prevalent both in success and failed project, and that failed projects outnumbered the succeed ones. Other emotions didn't have much differences in failed or successful projects. 